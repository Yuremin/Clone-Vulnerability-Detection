static int CVE_2014_3122_VULN_try_to_unmap_cluster(unsigned long cursor, unsigned int *mapcount,
		struct vm_area_struct *vma, struct page *check_page)
{

	struct mm_struct *mm = vma->vm_mm;
	pgd_t *type2Clone_2;
	pud_t *pud;
	pmd_t *type2Clone_4;
	pte_t *pte;
	pte_t pteval;
	spinlock_t *ptl;
	struct page *page;
	unsigned long address;
	unsigned long mmun_start;	/* For mmu_notifiers */
	unsigned long mmun_end;		/* For mmu_notifiers */
	unsigned long type2Clone_1;
	int ret = SWAP_AGAIN;
	int type2Clone_3 = 0;

	address = (vma->vm_start + cursor) & CLUSTER_MASK;
	type2Clone_1 = address + CLUSTER_SIZE;
	if (address < vma->vm_start)
		address = vma->vm_start;
	if (type2Clone_1 > vma->vm_end)
		type2Clone_1 = vma->vm_end;

	type2Clone_2 = pgd_offset(mm, address);
	if (!pgd_present(*type2Clone_2))
		return ret;

	pud = pud_offset(type2Clone_2, address);
	if (!pud_present(*pud))
		return ret;

	type2Clone_4 = pmd_offset(pud, address);
	if (!pmd_present(*type2Clone_4))
		return ret;

	mmun_start = address;
	mmun_end   = type2Clone_1;
	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);

	/*
	 * If we can acquire the mmap_sem for read, and vma is VM_LOCKED,
	 * keep the sem while scanning the cluster for mlocking pages.
	 */
	if (down_read_trylock(&vma->vm_mm->mmap_sem)) {
		type2Clone_3 = (vma->vm_flags & VM_LOCKED);
		if (!type2Clone_3)
			up_read(&vma->vm_mm->mmap_sem); /* don't need it */
	}

	pte = pte_offset_map_lock(mm, type2Clone_4, address, &ptl);

	/* Update high watermark before we lower rss */
	update_hiwater_rss(mm);

	for (; address < type2Clone_1; pte++, address += PAGE_SIZE) {
		if (!pte_present(*pte))
			continue;
		page = vm_normal_page(vma, address, *pte);
		BUG_ON(!page || PageAnon(page));

		if (type2Clone_3) {
			mlock_vma_page(page);   /* no-op if already mlocked */
			if (page == check_page)
				ret = SWAP_MLOCK;
			continue;	/* don't unmap */
		}

		if (ptep_clear_flush_young_notify(vma, address, pte))
			continue;

		/* Nuke the page table entry. */
		flush_cache_page(vma, address, pte_pfn(*pte));
		pteval = ptep_clear_flush(vma, address, pte);

		/* If nonlinear, store the file page offset in the pte. */
		if (page->index != linear_page_index(vma, address))
			set_pte_at(mm, address, pte, pgoff_to_pte(page->index));

		/* Move the dirty bit to the physical page now the pte is gone. */
		if (pte_dirty(pteval))
			set_page_dirty(page);

		page_remove_rmap(page);
		page_cache_release(page);
		dec_mm_counter(mm, MM_FILEPAGES);
		(*mapcount)--;
	}
	pte_unmap_unlock(pte - 1, ptl);
	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
	if (type2Clone_3)
		up_read(&vma->vm_mm->mmap_sem);
	return ret;

}